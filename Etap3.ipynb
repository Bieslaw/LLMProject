{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', sep=',', encoding='latin-1',  header=None)\n",
    "df.columns = [\"target\",\"ids\",\"date\",\"flag\",\"user\",\"text\"]\n",
    "df = df.drop([\"ids\",\"date\",\"flag\",\"user\"], axis = 1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaawansowane przetwarzanie i czyszczenie danych (2 pkt)\n",
    "- Wykorzystanie technik takich jak lematyzacja, stemming, uwzględnienie negacji.\n",
    "- Obsługa emotikonów, skrótów, błędów ortograficznych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize.casual import EMOTICON_RE\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "\n",
    "stop_words = pd.read_csv('stop_words.txt')\n",
    "stop_words = stop_words.loc[:,'word'].tolist()\n",
    "stopwords_simple = [word.replace(\"'\", \"\") for word in stop_words]\n",
    "stop_words = set(stop_words + stopwords_simple)\n",
    "\n",
    "misspelled_words = pd.read_csv('misspelled_words.txt',sep='->')\n",
    "misspelled_words = dict(zip(misspelled_words['miss'], misspelled_words['target']))\n",
    "\n",
    "def replace_consecutive_chars(s):\n",
    "    # Replace more than 3 consecutive '.' with exactly 3 of the same character\n",
    "    s = re.sub(r'([\\.])\\1{2,}', r'\\1\\1\\1', s)\n",
    "    \n",
    "    # Replace sequences of '!' and '?' with just one '!' or '?' respectively\n",
    "    s = re.sub(r'[!?]+(?:\\s*[!?]+)*', lambda m: m.group(0)[0], s)  # Replace with just the first character\n",
    "    \n",
    "    return s\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Filter out misspelled words\n",
    "    text = replace_consecutive_chars(text)\n",
    "    words = text.split()\n",
    "    words = [misspelled_words[word.lower()] if word.lower() in misspelled_words else word for word in words]\n",
    "    doc = nlp(' '.join(words))\n",
    "    # Tokenize, lemmatize, and filter out punctuation, special characters, lowercase\n",
    "    # Preserve emoticons\n",
    " \n",
    "    # Feature engineering\n",
    "    word_count = len([t for t in doc if not t.is_punct])\n",
    "    avg_word_length = sum(len(t.text) for t in doc) / word_count if word_count else 0\n",
    "    pos_counts = {pos: 0 for pos in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\", \"CCONJ\", \"ADP\", \"PROPN\", \"PUNCT\"]}\n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_counts:\n",
    "            pos_counts[token.pos_] += 1\n",
    "\n",
    "    words = [token.lemma_.lower() for token in doc if \n",
    "             (token.lemma_.isalpha() and token.lemma_.isascii() and not token.is_punct)\n",
    "             or EMOTICON_RE.match(token.text) or token.text in ['!', '?', '...']]\n",
    "    # Remove stopwords nicknames and links\n",
    "    words = [word for word in words if word not in stop_words] \n",
    "    words = [word for word in words if not word.startswith(('@', 'http', 'https', 'www.'))]\n",
    "    return {\n",
    "        \"tokens\": words,\n",
    "        **{f\"freq_{pos.lower()}\": pos_counts[pos] / word_count if word_count else 0 for pos in pos_counts},\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import swifter\n",
    "fname = 'tokenized.gzip'\n",
    "if os.path.isfile(fname):\n",
    "    print(\"Loading saved file\")\n",
    "    df = pd.read_pickle('tokenized.gzip', compression='gzip')\n",
    "else:\n",
    "    print(\"Preprocessing text\")\n",
    "    features = pd.DataFrame(df['text'].swifter.apply(lambda x: preprocess_text(x)).tolist(), index=df.index)\n",
    "    df.loc[df['target'] == 4, 'target'] = 1\n",
    "    df = pd.concat([df,features],axis=1)\n",
    "    # Save to file\n",
    "    df.to_pickle('tokenized.gzip',\n",
    "                compression='gzip') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib.pyplot import subplots\n",
    "positive_words = [word for words in df.loc[df['target'] == 1, 'tokens'].values.tolist() for word in words]\n",
    "negative_words = [word for words in df.loc[df['target'] == 0, 'tokens'].values.tolist() for word in words]\n",
    "pos_wc = WordCloud(max_words=100, width=1600, height=800, collocations=False).generate(' '.join(positive_words))\n",
    "neg_wc = WordCloud(max_words=100, width=1600, height=800, collocations=False).generate(' '.join(negative_words))\n",
    "\n",
    "fig, ax = subplots(nrows=2, ncols=1)\n",
    "ax[0].imshow(pos_wc)\n",
    "ax[1].imshow(neg_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inżynieria cech i reprezentacja tekstu (1,5 pkt)\n",
    "- Wykorzystanie embeddingu słów GloVe.\n",
    "- Tworzenie dodatkowych cech: analiza części mowy, średnia długośc słów (podczas przetwarzania danych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wektoryzacja "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Only the TensorFlow backend supports string inputs.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow.data as tf_data\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = train_df['tokens'].apply(len).max().item()\n",
    "print(max_length)\n",
    "vectorizer = layers.TextVectorization(max_tokens=20000, output_sequence_length=max_length)\n",
    "text_ds = tf_data.Dataset.from_tensor_slices(train_df['tokens'].apply(lambda x: ' '.join(x)).tolist()).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc,range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove embedding\n",
    "# Download https://nlp.stanford.edu/data/glove.twitter.27B.zip \n",
    "# and extract glove.twitter.27B.100d.txt file\n",
    "path_to_glove_file = \"glove.twitter.27B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "missed = []\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        missed.append(word)\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja zaawansowanego modelu (2 pkt)\n",
    "- Zastosowanie modeli takich jak sieci neuronowe (LSTM, CNN) lub transformatorów (BERT).\n",
    "- Uzasadnienie wyboru modelu i opis architektury."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uzasadnienie architektury CNN\n",
    "\n",
    "1. **Warstwa Embedding**: \n",
    "   - Służy do zamiany sekwencji indeksów słów na wektory osadzenia (embedding vectors), co pozwala na reprezentację semantyczną słów.\n",
    "   - Wykorzystuje wstępnie wytrenowaną macierz osadzeń (`embedding_matrix`).\n",
    "\n",
    "2. **Warstwy Conv1D**:\n",
    "   - Wykrywają lokalne wzorce w sekwencjach (np. n-gramy).\n",
    "   - Aktywacja `ReLU` wprowadza nieliniowość, umożliwiając modelowi naukę złożonych reprezentacji.\n",
    "\n",
    "3. **MaxPooling1D**:\n",
    "   - Redukuje rozmiar danych, zachowując najistotniejsze cechy, zapobiegając nadmiernemu dopasowaniu.\n",
    "\n",
    "4. **GlobalMaxPooling1D**:\n",
    "   - Wydobywa najważniejszą cechę z całej sekwencji, co jest przydatne w klasyfikacji tekstów.\n",
    "\n",
    "5. **Warstwa Dense**:\n",
    "   - Przekształca reprezentacje w bardziej złożone cechy.\n",
    "   - `ReLU` wprowadza nieliniowość.\n",
    "\n",
    "6. **Dropout**:\n",
    "   - Zapobiega nadmiernemu dopasowaniu (overfitting), poprawiając generalizację modelu.\n",
    "\n",
    "7. **Wyjście**:\n",
    "   - Warstwa `sigmoid` służy do klasyfikacji binarnej, generując wynik w przedziale [0, 1].\n",
    "\n",
    "8. **Kompilacja**:\n",
    "   - Funkcja straty: `binary_crossentropy` (klasyfikacja binarna).\n",
    "   - Optymalizator: `RMSprop`, często stosowany w zadaniach NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optymalizacja i tuning hiperparametrów (1 pkt)\n",
    "- Przeprowadzenie tuningu hiperparametrów Grid Search.\n",
    "- Uzasadnienie wyboru optymalnych parametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getX(df):\n",
    "    df2 = df.iloc[:,3:]\n",
    "    vect_data = vectorizer(\n",
    "        np.array(df[\"tokens\"].apply(lambda x: [\" \".join(x)]).tolist())\n",
    "    ).numpy()\n",
    "    feat_data = np.zeros(vect_data.shape)\n",
    "    feat_data[:, : df2.shape[1]] = df2.values\n",
    "    stacked = np.stack([vect_data, feat_data], axis=2)\n",
    "    return stacked\n",
    "\n",
    "\n",
    "x_train = getX(train_df)\n",
    "x_test = getX(test_df)\n",
    "\n",
    "y_train = train_df[\"target\"].to_numpy()\n",
    "y_test = test_df[\"target\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Custom function to merge inputs\n",
    "def prepare_inputs(x_embedding, x_features):\n",
    "    return [np.array(x_embedding), np.array(x_features)]\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "input_shape = (x_train.shape[1],x_train.shape[2])\n",
    "additional_features_size = df.iloc[:,3:].shape[1]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def extract_vector_input(x):\n",
    "    return x[:,:,0]\n",
    "@register_keras_serializable()\n",
    "def extract_feature_input(x):\n",
    "    return x[:,:additional_features_size,1]\n",
    "\n",
    "# Definicja funkcji do stworzenia modelu\n",
    "def create_model(filters, kernel_size, dense_units, dropout_rate,features_dense_units):\n",
    "    embedding_layer = Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        trainable=False,\n",
    "    )\n",
    "    embedding_layer.build((1,))\n",
    "    embedding_layer.set_weights([embedding_matrix])\n",
    "\n",
    "    # Combined input\n",
    "    combi_input = layers.Input(input_shape) \n",
    "    vectors = layers.Lambda(extract_vector_input, name=\"lambda_1\")(combi_input) \n",
    "    embedded_sequences = embedding_layer(vectors)\n",
    "    x = layers.Conv1D(filters, kernel_size, activation=\"relu\")(embedded_sequences)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "        # Additional feature input\n",
    "    additional_features_input = layers.Lambda(extract_feature_input, name=\"lambda_2\")(combi_input)\n",
    "    normalization = layers.Normalization()(additional_features_input)\n",
    "    y = layers.Dense(features_dense_units, activation=\"relu\")(normalization)\n",
    "    y = layers.Dense(1, activation=\"sigmoid\")(y)\n",
    "\n",
    "    # Combine embedding and additional features\n",
    "    combined = layers.concatenate([x, y])\n",
    "    z = layers.Dense(dense_units, activation=\"relu\")(combined)\n",
    "    z = layers.Dropout(dropout_rate)(z)\n",
    "    preds = layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = Model(combi_input, preds)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\", optimizer='rmsprop', metrics=[\"acc\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logika wczytywania lub tworzenia nowego modelu\n",
    "model_path = \"cnn_model_optimized.keras\"\n",
    "if os.path.isfile(model_path):\n",
    "    model = keras.models.load_model(model_path)\n",
    "else:\n",
    "    # Przygotowanie KerasClassifier do użycia z Random Search\n",
    "    model = KerasClassifier(\n",
    "        model=create_model,\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        dense_units=128,\n",
    "        dropout_rate=0.5,\n",
    "        verbose=0,\n",
    "        batch_size=64,\n",
    "        features_dense_units=16,\n",
    "    )\n",
    "\n",
    "    # Zakres hiperparametrów do przeszukania\n",
    "    param_distributions = {\n",
    "        \"filters\": [32, 64, 128],\n",
    "        \"kernel_size\": [2,3,4],\n",
    "        \"dense_units\": [32, 64, 128, 256],\n",
    "        \"dropout_rate\": [0.3, 0.5, 0.7],\n",
    "        \"batch_size\": [32, 64],\n",
    "        \"epochs\": [3],\n",
    "        \"features_dense_units\":[16,32,64]\n",
    "    }\n",
    "\n",
    "    # Przeprowadzenie Random Search\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_jobs=-1,\n",
    "        n_iter=2,\n",
    "        cv=3,\n",
    "        verbose=3,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Dopasowanie modelu do danych\n",
    "    search_result = search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = search_result.best_params_\n",
    "    print(\"Najlepsze parametry:\", best_params)\n",
    "    print(\"Najlepszy wynik:\", search_result.best_score_)\n",
    "    print(f\"Najlepsze parametry: {best_params}.\")\n",
    "\n",
    "    model = KerasClassifier(\n",
    "        model=create_model,\n",
    "        **best_params,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    model.model_.save(model_path)  # `model_.save` for scikeras wrapper\n",
    "if getattr(model, \"model_\", None) is not None:\n",
    "    model.model_.summary()\n",
    "else:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie z prostym modelem (0,5 pkt)\n",
    "- Analiza różnic w wynikach między modelami.\n",
    "- Wnioski dotyczące wpływu zaawansowanych technik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "vectorizer\n",
    "y_pred = model.predict(x_test)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = 0\n",
    "print(\"Dokładność modelu:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Raport klasyfikacji:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"ROC_AUC {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prosty model regresji logistycznej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_features=10000,\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "X_train_linear = tfidf.fit_transform(train_df['tokens'])\n",
    "X_test_linear = tfidf.transform(test_df['tokens'])\n",
    "Y_train_linear = train_df['target']\n",
    "Y_test_linear = test_df['target']\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter = 10000, n_jobs=-1).fit(X_train_linear, Y_train_linear)\n",
    "clf.score(X_train_linear, Y_train_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_linear)\n",
    "print(\"Dokładność modelu:\", accuracy_score(Y_test_linear, y_pred))\n",
    "print(\"Raport klasyfikacji:\\n\", classification_report(Y_test_linear, y_pred))\n",
    "roc_auc = roc_auc_score(Y_test_linear, clf.predict_proba(X_test_linear)[:, 1])\n",
    "print(f\"ROC_AUC {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
